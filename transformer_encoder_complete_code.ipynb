{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdcb9q/6oo3iW7XmyCquNY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/404saugat404/transformer/blob/main/transformer_encoder_complete_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qjD_bXxVLiC2"
      },
      "outputs": [],
      "source": [
        "#first lets define the parameteers that we will need\n",
        "d_model=512\n",
        "num_head=8\n",
        "drop_prob=0.1\n",
        "batch_size=30\n",
        "max_seq_len=200\n",
        "ffn_hidden=2048\n",
        "num_layer=5\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#before going on multihead attention, lets code for scaled dot product"
      ],
      "metadata": {
        "id": "yqgn_klQ3i3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scaled dot product\n",
        "\n",
        "def ScaledDotProduct(q,k,v,mask=None):\n",
        "  d_k=q.size()[-1]\n",
        "  scaled=torch.matmul(q,k.transpose(-1,-2))/torch.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled+=mask\n",
        "  attention=torch.softmax(scaled,dim=-1)\n",
        "  values=torch.matmul(attention,v)\n",
        "  return values,attention\n"
      ],
      "metadata": {
        "id": "-zbFxP3n3psy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for multihead attention"
      ],
      "metadata": {
        "id": "s8ksOtsqO_Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#multihead attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_head):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_head=num_head\n",
        "    self.head_dim=d_model//num_head\n",
        "    self.qkv_layer=nn.Linear(d_model,3*d_model)\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size,max_seq_len,d_model=x.size()\n",
        "    print(f\"x.size(): {x.size()}\")\n",
        "\n",
        "    qkv=self.qkv_layer(x)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    qkv=qkv.reshape(batch_size,max_seq_len,self.num_head,3*self.head_dim)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    qkv = qkv.permute(0, 2, 1, 3)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    q, k, v = qkv.chunk(3, dim=-1)\n",
        "    print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "\n",
        "    values, attention = ScaledDotProduct(q, k, v, mask)\n",
        "    print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "\n",
        "    values = values.reshape(batch_size, max_seq_len, d_model)\n",
        "    print(f\"values.size(): {values.size()}\")\n",
        "\n",
        "    output = self.linear_layer(values)\n",
        "    print(f\"output.size(): {output.size()}\")\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "XwddK4OFO_F9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for layer normalization"
      ],
      "metadata": {
        "id": "_rENUKATPe9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#layer normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "YfHyRLdQPeg3",
        "outputId": "2947d4c5-9dd6-43b4-d14b-48bb25cb5394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-10-691a2aa36a21>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-691a2aa36a21>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def __init__(self):\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for feed forward network"
      ],
      "metadata": {
        "id": "wFU9GsFVQ53S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):"
      ],
      "metadata": {
        "id": "7SyK5t6uQ22N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets define an EncoderLayer"
      ],
      "metadata": {
        "id": "zsd6TfIgN8TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EncoderLayer\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,num_head,drop_prob,ffn_hidden):\n",
        "    super.__init__()\n",
        "    self.attention=MultiHeadAttention(d_model=d_model,num_head=num_head)\n",
        "\n",
        "    self.norm1=LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    self.dropout1=nn.Dropout(p=drop_prob)\n",
        "\n",
        "    self.ffn=PositionWiseFeedForward(d_model=d_model,ffn_hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "\n",
        "\n",
        "\n",
        "    self.norm2=nn.LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    self.dropout2=nn.Dropout(p=drop_prob)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual_x=x\n",
        "\n",
        "    print(\"--------attention 1--------\")\n",
        "    x=self.attention(x,mask=None)\n",
        "\n",
        "    print(\"--------dropout--------\")\n",
        "    x=self.drorpout1(x)\n",
        "\n",
        "    print(\"--------add and norm--------\")\n",
        "    x=self.norm1(x+residual_x)\n",
        "\n",
        "    print(\"--------ffn--------\")\n",
        "    residual_x=x\n",
        "    x=self.ffn(x)\n",
        "\n",
        "    print(\"--------dropout2--------\")\n",
        "    x=self.drorpout2(x)\n",
        "\n",
        "    print(\"--------add and norm2--------\")\n",
        "    x=self.norm2(x+residual_x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "GYdyrXvkNhyK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets define the class encoder\n"
      ],
      "metadata": {
        "id": "mfmDpI3aL-JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,num_head,drop_prob,ffn_hidden,num_layers):\n",
        "    super.__init__()\n",
        "    self.layers=nn.Sequential(*[EncoderLayer(d_model,num_head,drop_prob,ffn_hidden) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.layers(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "BOoC74pDL5mn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets check the architecture\n",
        "encoder=Encoder(d_model,num_head,drop_prob,ffn_hidden,num_layer)\n",
        "x=torch.rand(batch_size,max_seq_len,d_model)\n",
        "output=encoder(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "dCtfKs_tN7VU",
        "outputId": "37f8c268-36b9-49c8-a65e-813ba78a71aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "descriptor '__init__' of 'super' object needs an argument",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1526eaeba5a2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#now lets check the architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mffn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9a4bfe959cac>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, num_head, drop_prob, ffn_hidden, num_layers)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mffn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mffn_hidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: descriptor '__init__' of 'super' object needs an argument"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CsqQsoKzO4nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}