{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2pu06yfaKf9lidJuhB83n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/404saugat404/transformer/blob/main/transformer_encoder_complete_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qjD_bXxVLiC2"
      },
      "outputs": [],
      "source": [
        "#first lets define the parameteers that we will need\n",
        "d_model=512\n",
        "num_head=8\n",
        "drop_prob=0.1\n",
        "batch_size=30\n",
        "max_seq_len=200\n",
        "ffn_hidden=2048\n",
        "num_layer=5\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#before going on multihead attention, lets code for scaled dot product"
      ],
      "metadata": {
        "id": "yqgn_klQ3i3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scaled dot product\n",
        "\n",
        "def ScaledDotProduct(q,k,v,mask=None):\n",
        "  d_k=q.size()[-1]\n",
        "  scaled=torch.matmul(q,k.transpose(-1,-2))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled+=mask\n",
        "  attention=torch.softmax(scaled,dim=-1)\n",
        "  values=torch.matmul(attention,v)\n",
        "  return values,attention\n"
      ],
      "metadata": {
        "id": "-zbFxP3n3psy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for multihead attention"
      ],
      "metadata": {
        "id": "s8ksOtsqO_Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#multihead attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_head):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.num_head=num_head\n",
        "    self.head_dim=d_model//num_head\n",
        "    self.qkv_layer=nn.Linear(d_model,3*d_model)\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size,max_seq_len,d_model=x.size()\n",
        "    print(f\"x.size(): {x.size()}\")\n",
        "\n",
        "    qkv=self.qkv_layer(x)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    qkv=qkv.reshape(batch_size,max_seq_len,self.num_head,3*self.head_dim)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    qkv = qkv.permute(0, 2, 1, 3)\n",
        "    print(f\"qkv.size(): {qkv.size()}\")\n",
        "\n",
        "    q, k, v = qkv.chunk(3, dim=-1)\n",
        "    print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "\n",
        "    values, attention = ScaledDotProduct(q, k, v, mask)\n",
        "    print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "\n",
        "    values = values.reshape(batch_size, max_seq_len, d_model)\n",
        "    print(f\"values.size(): {values.size()}\")\n",
        "\n",
        "    output = self.linear_layer(values)\n",
        "    print(f\"output.size(): {output.size()}\")\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "XwddK4OFO_F9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for layer normalization"
      ],
      "metadata": {
        "id": "_rENUKATPe9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#layer normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape=parameters_shape\n",
        "    self.eps=eps\n",
        "    self.gamma=nn.Parameter(torch.ones(parameters_shape))\n",
        "    self.beta=nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "    mean = inputs.mean(dim=dims, keepdim=True)\n",
        "    print(f\"Mean ({mean.size()})\")\n",
        "    var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "    std = (var + self.eps).sqrt()\n",
        "    print(f\"Standard Deviation  ({std.size()})\")\n",
        "    y = (inputs - mean) / std\n",
        "    print(f\"y: {y.size()}\")\n",
        "    output = self.gamma * y  + self.beta\n",
        "    print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "    print(f\"out: {output.size()}\")\n",
        "    return output\n",
        "\n",
        "\n",
        "    #try to understand this in more detail way using chatgpt or youtube\n"
      ],
      "metadata": {
        "id": "YfHyRLdQPeg3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets create a class for feed forward network"
      ],
      "metadata": {
        "id": "wFU9GsFVQ53S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,hidden,drop_prob):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.linear1=nn.Linear(d_model,hidden)\n",
        "    self.linear2=nn.Linear(hidden,d_model)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.dropout=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.linear1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.linear2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "7SyK5t6uQ22N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets define an EncoderLayer"
      ],
      "metadata": {
        "id": "zsd6TfIgN8TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EncoderLayer\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,num_head,drop_prob,ffn_hidden):\n",
        "    super().__init__()\n",
        "    self.attention=MultiHeadAttention(d_model=d_model,num_head=num_head)\n",
        "\n",
        "    self.norm1=LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    self.dropout1=nn.Dropout(p=drop_prob)\n",
        "\n",
        "    self.ffn=PositionWiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "\n",
        "\n",
        "\n",
        "    self.norm2=LayerNormalization(parameters_shape=[d_model])\n",
        "\n",
        "    self.dropout2=nn.Dropout(p=drop_prob)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual_x=x\n",
        "\n",
        "    print(\"--------attention 1--------\")\n",
        "    x=self.attention(x,mask=None)\n",
        "\n",
        "    print(\"--------dropout--------\")\n",
        "    x=self.dropout1(x)\n",
        "\n",
        "    print(\"--------add and norm--------\")\n",
        "    x=self.norm1(x+residual_x)\n",
        "\n",
        "    print(\"--------ffn--------\")\n",
        "    residual_x=x\n",
        "    x=self.ffn(x)\n",
        "\n",
        "    print(\"--------dropout2--------\")\n",
        "    x=self.dropout2(x)\n",
        "\n",
        "    print(\"--------add and norm2--------\")\n",
        "    x=self.norm2(x+residual_x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "GYdyrXvkNhyK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lets define the class encoder\n"
      ],
      "metadata": {
        "id": "mfmDpI3aL-JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,num_head,drop_prob,ffn_hidden,num_layers):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(*[EncoderLayer(d_model,num_head,drop_prob,ffn_hidden) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.layers(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "BOoC74pDL5mn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets check the architecture\n",
        "encoder=Encoder(d_model,num_head,drop_prob,ffn_hidden,num_layer)\n",
        "x=torch.rand(batch_size,max_seq_len,d_model)\n",
        "output=encoder(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCtfKs_tN7VU",
        "outputId": "126c0b2b-c104-4a2b-ba1c-91ecbcc8365b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = torch.randn( (batch_size, max_seq_len, d_model) ) # includes positional encoding\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsqQsoKzO4nl",
        "outputId": "4dfaeb12-1c25-4423-f010-8de68aa43dc0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------attention 1--------\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv.size(): torch.Size([30, 200, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 200, 192])\n",
            "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
            "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
            "values.size(): torch.Size([30, 200, 512])\n",
            "output.size(): torch.Size([30, 200, 512])\n",
            "--------dropout--------\n",
            "--------add and norm--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "--------ffn--------\n",
            "--------dropout2--------\n",
            "--------add and norm2--------\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VH9_OEV99JHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}